{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "otu_table = pd.read_csv(\"/data/namlhs/omics-data-learners/data/metsim\"\n",
    "                        \"/01_raw/clinical_data/formatted/OTUS.txt\", \n",
    "                        delim_whitespace=True,\n",
    "                        index_col=0)\n",
    "\n",
    "tax_table = pd.read_csv(\"/data/namlhs/omics-data-learners/data/metsim\"\n",
    "                        \"/01_raw/clinical_data/formatted/TAXTABLE.txt\", \n",
    "                        delim_whitespace=True,\n",
    "                        index_col=0)\n",
    "\n",
    "pre_df = pd.read_csv('data/metsim/01_raw/clinical_data/formatted/FINAL_MICROBIOME_DATASET.csv', \n",
    "                 index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df[\"keep\"] = 0\n",
    "\n",
    "# For rows that prediabetes = 1, dm = 0, and DMType = 1, set the \"keep\" column to 1\n",
    "for index, row in pre_df.iterrows():\n",
    "    if row[\"DMType\"] == 2 and row[\"dm\"] == 0:\n",
    "        pre_df.loc[index, \"keep\"] = 1\n",
    "\n",
    "# Keep only rows where \"keep\" is equal to 1\n",
    "cases_df = pre_df.loc[pre_df[\"keep\"] == 1]\n",
    "patient = cases_df['METSIM_ID']\n",
    "\n",
    "\n",
    "# control data frame\n",
    "control_df = pre_df.loc[pre_df['METSIM_ID'].isin(patient) == False]\n",
    "\n",
    "# Group the data frame by METSIM_ID\n",
    "control_df = control_df.groupby(\"METSIM_ID\")\n",
    "\n",
    "# Select only the row that has the lowest time_point for each METSIM_ID\n",
    "control_df = control_df.apply(lambda x: x.sort_values(\"Time_Point\").iloc[0])\n",
    "\n",
    "control_df[\"keep\"] = 1\n",
    "\n",
    "# For rows that prediabetes = 1, dm = 0, and DMType = 1, set the \"keep\" column to 1\n",
    "for index, row in control_df.iterrows():\n",
    "    if row[\"DMType\"] == 2 and row[\"dm\"] == 1:\n",
    "        control_df.loc[index, \"keep\"] = 0\n",
    "\n",
    "control_df = control_df.loc[control_df[\"keep\"] == 1]\n",
    "\n",
    "df = pd.concat([cases_df, control_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers_genus = [\n",
    "    # 'Akkermansia', \n",
    "    # 'Oscillospira',\n",
    "    'Bacteroides',\n",
    "    # 'Clostridium',\n",
    "    # 'Eggerthella',\n",
    "    # 'Escherichia',\n",
    "    'Faecalibaterium',\n",
    "    'Firmicutes',\n",
    "    # 'Veillonella'\n",
    "    ]\n",
    "markers_species = [\n",
    "    # 'vulgatus', \n",
    "    # 'ovatus', \n",
    "    # 'muciniphila', \n",
    "    # 'prausnitzii', \n",
    "    # 'salivarius',\n",
    "    # 'coli'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#relative abundance\n",
    "otu_rel_table = (otu_table.T/otu_table.sum(axis=1)).T\n",
    "otu_rel_table.sum(axis=1)\n",
    "\n",
    "def create_array(start, end, prefix):\n",
    "    return np.array([prefix + str(i) for i in range(start, end + 1)])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    top_abund = create_array(1, 15, \"ASV\")\n",
    "    lim_abund = create_array(1, 200, \"ASV\")\n",
    "    print(top_abund)\n",
    "    print(lim_abund)\n",
    "\n",
    "# pick most abundance and \n",
    "pick_otu = tax_table[((tax_table['Species'].isin(markers_species)) |\n",
    "                      (tax_table['Genus'].isin(markers_genus)) |\n",
    "                      (tax_table.index.isin(top_abund))) &\n",
    "                     (tax_table.index.isin(lim_abund))].index\n",
    "\n",
    "otu_fil = otu_rel_table[pick_otu]\n",
    "otu_fil.index = otu_fil.index.str.replace('_', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the data sparsity\n",
    "sparse = df.isnull().sum()/len(df)\n",
    "display(sparse)\n",
    "\n",
    "sparse_filtered = sparse[sparse < 0.2]\n",
    "\n",
    "# Display the filtered Series\n",
    "print(sparse_filtered)\n",
    "\n",
    "#keep only column in sparse_filtered\n",
    "df_filtered = df.loc[:, sparse_filtered.index]\n",
    "df_filtered = df_filtered.set_index('SampleID')\n",
    "display(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df = df_filtered[[\n",
    "    'DMType', \n",
    "    'Age', 'WHR', 'fmass',\n",
    "    'BMI', 'Freq_veg', 'Freq_fruit',\n",
    "    'Freq_leanfish',\n",
    "    'Freq_strongwine', 'Freq_blend',\n",
    "    'Freq_wine', 'Freq_alclt3', 'Freq_alclt6', 'Freq_alcge6', 'Freq_liqueur',\n",
    "    'Milk',\n",
    "    'Spread_no', 'Spread_marg',\n",
    "    'Cookfat_sat', 'Cookfat_no', 'Cookfat_marg', 'Cookfat_oils',\n",
    "    'Redmeat_gwk',\n",
    "    'Cheese_freq', 'Cheese_g', 'Cheese_other',\n",
    "    'Cereal_24_serv_wholegrain','Cereal_24_serv_pastry'\n",
    "    ]].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge data frame into 1\n",
    "match_df = pd.merge(alt_df, otu_fil, left_index=True, right_index=True)\n",
    "\n",
    "# meta_df = match_df.drop(columns=['dm', 'METSIM_ID', 'Time_Point'])\n",
    "meta_df = match_df\n",
    "\n",
    "df_cor = meta_df.corr(method='kendall')\n",
    "df_pairs = df_cor.unstack()\n",
    "\n",
    "# print(df_pairs)\n",
    "sorted_pairs = df_pairs.sort_values(kind='quicksort')\n",
    "remove_pairs = sorted_pairs[(abs(sorted_pairs) >= 0.5) & (sorted_pairs != 1)].reset_index()\n",
    "\n",
    "display(remove_pairs)\n",
    "\n",
    "#Check the NaN values\n",
    "print(meta_df.isnull().sum())\n",
    "\n",
    "# Get the list of columns to drop\n",
    "drop_cols = list(remove_pairs['level_0'])\n",
    "\n",
    "# Drop the columns from meta_df\n",
    "chosen_df = meta_df.drop(columns=[])\n",
    "\n",
    "#for combine data\n",
    "alt_data = chosen_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"distance\", metric='nan_euclidean')\n",
    "\n",
    "array_imputed = imputer.fit_transform(alt_data)\n",
    "#print(alt_df.columns)\n",
    "df_imputed = pd.DataFrame(array_imputed, columns=chosen_df.columns)\n",
    "\n",
    "#Check the NaN values\n",
    "print(df_imputed.isnull().sum())\n",
    "\n",
    "df_imputed['DMType'].loc[(df_imputed['DMType'] > 0)] = 1\n",
    "display(df_imputed)\n",
    "df_imputed['DMType'].value_counts()\n",
    "df_imputed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "X = df_imputed.drop(columns ='DMType')\n",
    "\n",
    "y = df_imputed.DMType\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Scale\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_scale = scaler.transform(X)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = np.zeros(len(y_train))\n",
    "sample_weights[y_train == 0] = 0.2\n",
    "sample_weights[y_train == 1] = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# # creating a RF classifier\n",
    "# clf = RandomForestClassifier(n_estimators = 1000,\n",
    "#                              class_weight='balanced',\n",
    "#                              max_depth=4,\n",
    "#                              max_features=None,\n",
    "#                              random_state=777)  \n",
    "  \n",
    "# # Training the model on the training dataset\n",
    "# # fit function is used to train the model using the training sets as parameters\n",
    "# clf.fit(X_train, y_train)\n",
    "  \n",
    "# # performing predictions on the test dataset\n",
    "# y_pred = clf.predict(X_test)\n",
    "  \n",
    "# # metrics are used to find accuracy or error\n",
    "# from sklearn import metrics\n",
    "\n",
    "# # using metrics module for accuracy calculation\n",
    "# print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "lr_list = [0.005, 0.01, 0.02, 0.05, 0.1, 0.25, 0.5, 1]\n",
    "\n",
    "for learning_rate in lr_list:\n",
    "    gb_clf = GradientBoostingClassifier(n_estimators=20, learning_rate=learning_rate, max_features=200, max_depth=2, random_state=0)\n",
    "    gb_clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Learning rate: \", learning_rate)\n",
    "    print(\"Accuracy score (training): {0:.3f}\".format(gb_clf.score(X_train, y_train)))\n",
    "    print(\"Accuracy score (validation): {0:.3f}\".format(gb_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the metrics class\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "gb_clf2 = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5, max_features=200, max_depth=2, random_state=0)\n",
    "gb_clf2.fit(X_train, y_train)\n",
    "y_pred = gb_clf2.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "target_names = ['without diabetes', 'with diabetes']\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# confusion matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix\n",
    "\n",
    "# Visualize\n",
    "# import required modules\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.rcParams['figure.facecolor'] = '#f2f2f2'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
