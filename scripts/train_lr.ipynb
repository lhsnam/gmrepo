{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span><h1 style = \"font-family: montserrat; font-size: 50px; font-style: normal; letter-spcaing: 3px; background-color: #f1faee; color :#1d3557; border-radius: 10px 10px; text-align:center\"> **Model training** <br> </span> <span style = \"font-family: montserrat; font-size: 35px\"> for exploring microbiome data </h1> <span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3861651/3418884620.py:4: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  otu_table = pd.read_csv(\"/data/namlhs/omics-data-learners/data/metsim\"\n",
      "/tmp/ipykernel_3861651/3418884620.py:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  tax_table = pd.read_csv(\"/data/namlhs/omics-data-learners/data/metsim\"\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import pandas as pd\n",
    "\n",
    "otu_table = pd.read_csv(\"/data/namlhs/omics-data-learners/data/metsim\"\n",
    "                        \"/01_raw/clinical_data/formatted/OTUS.txt\", \n",
    "                        delim_whitespace=True,\n",
    "                        index_col=0)\n",
    "\n",
    "tax_table = pd.read_csv(\"/data/namlhs/omics-data-learners/data/metsim\"\n",
    "                        \"/01_raw/clinical_data/formatted/TAXTABLE.txt\", \n",
    "                        delim_whitespace=True,\n",
    "                        index_col=0)\n",
    "\n",
    "df = pd.read_csv('/data/namlhs/omics-data-learners/data/metsim/01_raw/clinical_data/formatted/FINAL_MICROBIOME_DATASET.csv', \n",
    "                 index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ASV1    ASV2    ASV3    ASV4    ASV5   ASV6  ASV7  ASV8   ASV9  \\\n",
      "Sample_ID                                                                     \n",
      "MET_0001   279118  288094    1175       0   15601  11118  1119   238   1346   \n",
      "MET_0002   195256  103634  344333   29106   27040   4575   488   328      0   \n",
      "MET_0003    12473   28944   61544  196210  226003  67630   355   379  44977   \n",
      "MET_0005    54922   18382    6742    8003   39117  47958   299   304   9609   \n",
      "MET_0006    72576   76530   54979    4992   27167   9310   232   166  33013   \n",
      "\n",
      "           ASV10  ...  ASV23910  ASV23911  ASV23912  ASV23913  ASV23914  \\\n",
      "Sample_ID         ...                                                     \n",
      "MET_0001     524  ...         0         0         0         0         0   \n",
      "MET_0002   60996  ...         0         0         0         0         0   \n",
      "MET_0003    8108  ...         0         0         0         0         0   \n",
      "MET_0005   13679  ...         0         0         0         0         0   \n",
      "MET_0006   11600  ...         0         0         0         0         0   \n",
      "\n",
      "           ASV23915  ASV23916  ASV23917  ASV23918  ASV23919  \n",
      "Sample_ID                                                    \n",
      "MET_0001          0         0         0         0         0  \n",
      "MET_0002          0         0         0         0         0  \n",
      "MET_0003          0         0         0         0         0  \n",
      "MET_0005          0         0         0         0         0  \n",
      "MET_0006          0         0         0         0         0  \n",
      "\n",
      "[5 rows x 23919 columns]\n"
     ]
    }
   ],
   "source": [
    "print(otu_table.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relative abundance\n",
    "otu_rel_table = (otu_table.T/otu_table.sum(axis=1)).T\n",
    "otu_rel_table.sum(axis=1)\n",
    "\n",
    "# pick first 50 ASVs only\n",
    "otu_fil = otu_rel_table.iloc[: , :51]\n",
    "\n",
    "otu_fil.index = otu_fil.index.str.replace('_', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the data sparsity\n",
    "sparse = df.isnull().sum()/len(df)\n",
    "display(sparse)\n",
    "\n",
    "sparse_filtered = sparse[sparse < 0.2]\n",
    "\n",
    "# Display the filtered Series\n",
    "print(sparse_filtered)\n",
    "\n",
    "#keep only column in sparse_filtered\n",
    "df_filtered = df.loc[:, sparse_filtered.index]\n",
    "df_filtered = df_filtered.set_index('SampleID')\n",
    "display(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df = df_filtered[['dm', 'METSIM_ID', 'Time_Point',\n",
    "                      'Age', 'DMType', 'WHR',\n",
    "                      'fmass', 'diastbp', 'systbp',\n",
    "                      'BMI', 'Freq_veg', 'Freq_fruit',\n",
    "                      'Freq_leanfish', 'Freq_fattyfish', 'Freq_shellfish',\n",
    "                      'Freq_strongwine', 'Freq_blend',\n",
    "                      'Freq_wine', 'Freq_alclt3', 'Freq_alclt6', 'Freq_alcge6', 'Freq_liqueur',\n",
    "                      'Milk', 'Milk_quantity', 'Dairy_other',\n",
    "                      'Spread_sat', 'Spread_no', 'Spread_marg',\n",
    "                      'Cookfat_sat', 'Cookfat_no', 'Cookfat_marg', 'Cookfat_oils',\n",
    "                      'Redmeat_gwk',\n",
    "                      'Cheese_freq', 'Cheese_g', 'Cheese_gvko', 'Cheese_other',\n",
    "                      'Cereal_24_serv_wholegrain', 'Cereal_24_serv_wheat',\n",
    "                      'Cereal_24_serv_pastry']].copy()\n",
    "\n",
    "#merge first 50 ASVs relative abundance\n",
    "match_df = pd.merge(alt_df, otu_fil, left_index=True, right_index=True)\n",
    "meta_df = match_df.drop(columns=['dm', 'METSIM_ID', 'Time_Point'])\n",
    "df_cor = meta_df.corr(method='kendall')\n",
    "df_pairs = df_cor.unstack()\n",
    "\n",
    "# print(df_pairs)\n",
    "sorted_pairs = df_pairs.sort_values(kind='quicksort')\n",
    "remove_pairs = sorted_pairs[(abs(sorted_pairs) >= 0.5) & (sorted_pairs != 1)]\n",
    "\n",
    "display(remove_pairs)\n",
    "\n",
    "#Check the NaN values\n",
    "print(meta_df.isnull().sum())\n",
    "\n",
    "# consider remove 'Spread_marg', 'Milk_quantity', 'Cheese_g'\n",
    "\n",
    "chosen_df = meta_df.drop(columns = ['Spread_marg', 'Milk_quantity', 'Cheese_g'])\n",
    "alt_data = chosen_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"distance\", metric='nan_euclidean')\n",
    "\n",
    "array_imputed = imputer.fit_transform(alt_data)\n",
    "#print(alt_df.columns)\n",
    "df_imputed = pd.DataFrame(array_imputed, columns=chosen_df.columns)\n",
    "\n",
    "#Check the NaN values\n",
    "print(df_imputed.isnull().sum())\n",
    "\n",
    "df_imputed['DMType'].loc[(df_imputed['DMType'] > 0)] = 1\n",
    "display(df_imputed)\n",
    "df_imputed['DMType'].value_counts()\n",
    "df_imputed.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "X = df_imputed.drop(columns ='DMType')\n",
    "\n",
    "y = df_imputed.DMType\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Scale\u001b[39;00m\n\u001b[1;32m      6\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m----> 7\u001b[0m scaler\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m)\n\u001b[1;32m      9\u001b[0m X_scale \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m     11\u001b[0m X_train \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_scale = scaler.transform(X)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression(random_state=777, \n",
    "                            max_iter= 100000, \n",
    "                            class_weight= 'balanced',\n",
    "                            penalty=\"elasticnet\", \n",
    "                            solver=\"saga\",\n",
    "                            C=1000,\n",
    "                            l1_ratio=0.15,\n",
    "                            )\n",
    "\n",
    "lr_train = logreg.fit(X_train, y_train)\n",
    "y_pred = lr_train.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotnine as p9\n",
    "\n",
    "# get the feature coefficients and feature names\n",
    "feature_coef = lr_train.coef_[0]\n",
    "feature_names = X.columns.tolist() # assuming your input data is a pandas DataFrame\n",
    "\n",
    "coef_df = pd.DataFrame({'factors':feature_names, 'coef':feature_coef}).sort_values(ascending=False, by=\"coef\")\n",
    "display(coef_df)\n",
    "\n",
    "#sorted factor\n",
    "factor_list = coef_df['factors']\n",
    "\n",
    "(\n",
    "    p9.ggplot(coef_df, p9.aes(x = 'factors', y = 'coef')) +\n",
    "    p9.geom_col() +\n",
    "    p9.scale_x_discrete(limits = factor_list) +\n",
    "    p9.coord_flip()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the metrics class\n",
    "from sklearn import metrics\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix\n",
    "\n",
    "# Visualize\n",
    "# import required modules\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.rcParams['figure.facecolor'] = '#f2f2f2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['without diabetes', 'with diabetes']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = logreg.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify longitudinal patients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilities check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = logreg.predict_proba(X_scale)\n",
    "proba_pos = proba[:, 1]\n",
    "\n",
    "match_df.loc[:, 'Proba'] = proba_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df[(match_df['DMType'] == 0) & (match_df['Proba'] >= 0.5)].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df['DMType'] = match_df['DMType'].fillna(0)\n",
    "match_df['DMType'] = match_df['DMType'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.tools import mpl_to_plotly\n",
    "\n",
    "proba_plot = (p9.ggplot(data = match_df, \n",
    "                        mapping = p9.aes(x = 'Time_Point',\n",
    "                                         y = 'Proba')) +\n",
    "                  p9.geom_line(p9.aes(group = 'METSIM_ID'),\n",
    "                                                      alpha = 0.3) +\n",
    "                  p9.geom_point(p9.aes(color = 'DMType')) +\n",
    "                  p9.scale_color_discrete(labels = ['No', 'Yes']) + \n",
    "                  p9.labs(color = 'Diabetes',\n",
    "                          x = 'Time Point',\n",
    "                          y = 'Diabetes Probability') +\n",
    "                  p9.ylim(0,1) +\n",
    "                  p9.theme(figure_size=(5, 5))\n",
    "            )\n",
    "\n",
    "proba_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_df = match_df[(match_df['dm'] == 1) | (match_df['DMType'] == 2)]\n",
    "patient_id = cases_df['METSIM_ID'].unique()\n",
    "\n",
    "patient_df = match_df[match_df['METSIM_ID'].isin(patient_id)]\n",
    "\n",
    "patient_df.to_csv(r'/data/namlhs/visualization/t2d_probs.csv', \n",
    "                  columns=['METSIM_ID', 'Time_Point', 'Proba', 'DMType'],\n",
    "                  sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diab_plot = (p9.ggplot(data = patient_df, \n",
    "                        mapping = p9.aes(x = 'Time_Point',\n",
    "                                         y = 'Proba')) +\n",
    "                  p9.geom_line(p9.aes(group = 'METSIM_ID'),\n",
    "                                                      alpha = 0.3) +\n",
    "                  p9.geom_point(p9.aes(color = 'DMType')) +\n",
    "                  p9.scale_color_discrete(labels = ['No', 'Yes']) + \n",
    "                  p9.labs(color = 'Diabetes',\n",
    "                          x = 'Time Point',\n",
    "                          y = 'Diabetes Probability') +\n",
    "                  p9.ylim(0,1) +\n",
    "                  p9.theme(figure_size=(7, 7))\n",
    "            )\n",
    "\n",
    "diab_plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
